<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Gauri Chaudhari</title>
    <link>https://gaurichaudhari9.github.io/project.html</link>
      <atom:link href="https://gaurichaudhari9.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en</language><copyright>Â© 2024 Gauri Chaudhari</copyright><lastBuildDate>Wed, 03 Jan 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Projects</title>
      <link>https://gaurichaudhari9.github.io/project.html</link>
    </image>
    
    <item>
      <title>Interactive Housing Sales Analytics Dashboard</title>
      <link>https://gaurichaudhari9.github.io/project/interactive-housing-sales-analytics-dashboard.html</link>
      <pubDate>Wed, 03 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://gaurichaudhari9.github.io/project/interactive-housing-sales-analytics-dashboard.html</guid>
      <description>&lt;p&gt;This dashboard visualizes and analyzes housing sales data for King County, Washington. The dashboard features a calendar widget, line chart, map, histograms, and a heatmap, providing a user-friendly interface to explore the dataset. The purpose is to understand trends in housing prices and identify relationships between price and other attributes like number of bedrooms/bathrooms, square footage, view, condition, etc.&lt;/p&gt;
&lt;p&gt;The dashboard allows users to filter the data by month and day to see how prices change over time. Additional filters for year built, square feet living area, and square feet lot area allow further slicing of the data.&lt;/p&gt;
&lt;h2 id=&#34;analysis&#34;&gt;Analysis&lt;/h2&gt;
&lt;p&gt;The dashboard consists of the following visualizations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Line chart showing average price over time&lt;/li&gt;
&lt;li&gt;Map showing average price by ZIP code, colored by price&lt;/li&gt;
&lt;li&gt;Histogram showing distribution of house prices&lt;/li&gt;
&lt;li&gt;Histograms for distribution of number of bedrooms and bathrooms&lt;/li&gt;
&lt;li&gt;Heatmap showing average price by view and condition&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The visualizations allow us to identify trends like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prices increasing over time&lt;/li&gt;
&lt;li&gt;Areas with higher concentrations of expensive houses&lt;/li&gt;
&lt;li&gt;Most houses having 2-4 bedrooms and 1-3 bathrooms&lt;/li&gt;
&lt;li&gt;Higher prices for better views and conditions&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;instructions&#34;&gt;Instructions&lt;/h2&gt;
&lt;p&gt;The dashboard can be filtered in the following ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Select month and day from calendar to filter all visualizations to that time period&lt;/li&gt;
&lt;li&gt;Use sliders for year built, square feet living area, and square feet lot area to filter the bottom 4 visualizations&lt;/li&gt;
&lt;li&gt;Hover over marks in the visualizations to see more details&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The dashboard is interactive and allows slicing data in different ways to uncover insights.&lt;/p&gt;
&lt;h2 id=&#34;dashboard&#34;&gt;Dashboard&lt;/h2&gt;
&lt;p&gt;To learn more about the dashboard click on the github icon at the top and to view the dashboard click on the tableau button at the top.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Classifying TV Show Quotes using BERT</title>
      <link>https://gaurichaudhari9.github.io/project/classifying-tv-show-quotes-with-bert.html</link>
      <pubDate>Fri, 08 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://gaurichaudhari9.github.io/project/classifying-tv-show-quotes-with-bert.html</guid>
      <description>&lt;p&gt;I worked on an NLP classification project to distinguish quotes from two popular TV shows - Star Wars and Friends. The goal was to train a model to accurately categorize unseen quotes as belonging to one show or the other.&lt;/p&gt;
&lt;h2 id=&#34;the-adventure-begins&#34;&gt;The Adventure Begins&amp;hellip;&lt;/h2&gt;
&lt;p&gt;Every good quest starts with assembling a crew. I needed to gather a dataset of quotes from the two shows. This required venturing out into the wilderness of the internet to scrape relevant quotes from fan sites using &lt;strong&gt;selenium&lt;/strong&gt; and &lt;strong&gt;beautifulSoup&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;After gathering over 100 quotes from each show, it was time to &lt;strong&gt;wrangle the unruly data&lt;/strong&gt; into a form suitable for ML models. I cleaned and preprocessed the quotes, adding labels to denote the source show.
python&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;quotes[&#39;label&#39;] = 0 # Star Wars
quotes[&#39;label&#39;] = 1 # Friends
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;training-time&#34;&gt;Training Time&lt;/h2&gt;
&lt;p&gt;Now for the fun part - &lt;strong&gt;teaching a model to discern quote origins&lt;/strong&gt;! I turned to the mighty BERT architecture and leveraged a pretrained model. BERT has oozed NLP knowledge from consuming piles of text data. I just needed to fine-tune it with my quotes.&lt;/p&gt;
&lt;p&gt;First, I had to &lt;strong&gt;encode the text&lt;/strong&gt; into numeric tokens that BERT digests:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tokenizer = BertTokenizer.from_pretrained(&#39;bert-base-uncased&#39;)

encoded_quotes = tokenizer(quotes, 
                           padding=True,
                           truncation=True,
                           return_tensors=&#39;tf&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I compiled a BERT model, fitted it on the training quotes, and evaluated it on a held-out test set.&lt;/p&gt;
&lt;h2 id=&#34;the-journeys-end&#34;&gt;The Journey&amp;rsquo;s End&lt;/h2&gt;
&lt;p&gt;After fine-tuning the BERT model on the quote dataset, it was time to evaluate its performance and see how well it learned to classify the quotes.
I tested the model on a held-out set of 40 quotes that it had never seen before - 20 Star Wars and 20 Friends. This would give an unbiased estimate of its real-world accuracy.
The model achieved an overall test accuracy of &lt;strong&gt;80%!&lt;/strong&gt; This exceeded my expectations considering the small dataset size.
Digging deeper into the results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Precision for Star Wars quotes was 82% and Friends 78%&lt;/li&gt;
&lt;li&gt;Recall for Star Wars was 85% and Friends 75%
The confusion matrix showed the model struggled slightly more with Friends quotes - mistaking 5 of them for Star Wars.&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;This may be because Star Wars quotes have more distinct language like &amp;ldquo;Use the force&amp;rdquo; and mentions of &amp;ldquo;Jedi&amp;rdquo;. Whereas Friends has more everyday conversational quotes that are harder to classify.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Overall though, BERT was successfully able to learn distinguishing features between the two classes with moderate success.&lt;/p&gt;
&lt;p&gt;There is definitely scope to improve accuracy further with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A larger dataset for better generalization&lt;/li&gt;
&lt;li&gt;Trying different pretrained models like RoBERTa&lt;/li&gt;
&lt;li&gt;Hyperparameter tuning - learning rate, epochs etc.&lt;/li&gt;
&lt;li&gt;Regularization techniques like dropout to reduce overfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But as a first attempt, I am pleased with the model&amp;rsquo;s capabilities. It can classify unseen quotes with 80% accuracy - not bad!
This was a fun first adventure in training BERT for text classification. I&amp;rsquo;m excited to keep exploring how transformers can be applied to NLP tasks going forward.&lt;/p&gt;
&lt;p&gt;Until next time, may the force be with you!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Investigating Transfer Learning and Scratch Training for Image Recognition</title>
      <link>https://gaurichaudhari9.github.io/project/investigating-transfer-learning-and-scratch-training-for-image-recognition.html</link>
      <pubDate>Fri, 08 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://gaurichaudhari9.github.io/project/investigating-transfer-learning-and-scratch-training-for-image-recognition.html</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This project explores image classification using convolutional neural networks (CNNs). The goal is to classify images into three categories - apparel, chairs, and footwear.
The project covers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data collection and labeling&lt;/li&gt;
&lt;li&gt;Splitting data into train, validation, and test sets&lt;/li&gt;
&lt;li&gt;Building input pipelines with data augmentation&lt;/li&gt;
&lt;li&gt;Fine-tuning a pretrained CNN model (ResNet50)&lt;/li&gt;
&lt;li&gt;Training a CNN from scratch&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;data-collection-and-labeling&#34;&gt;Data Collection and Labeling&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Collected over 100 images per class (apparel, chairs, footwear) using a phone camera&lt;/li&gt;
&lt;li&gt;Labeled images and assigned category labels - 0 for apparel, 1 for chairs, 2 for footwear&lt;/li&gt;
&lt;/ul&gt;
&lt;img width=&#34;950&#34; alt=&#34;image&#34; src=&#34;https://github.com/gaurichaudhari9/image-classification-cnn/assets/25304556/2ca49bb5-52c8-45b3-b3dc-119e6ae70705&#34;&gt;
&lt;h2 id=&#34;data-splitting&#34;&gt;Data Splitting&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Split data into 60% train, 20% validation, 20% test per class&lt;/li&gt;
&lt;li&gt;Copied split data into separate folders for train, validation, and test&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;input-pipeline-and-data-augmentation&#34;&gt;Input Pipeline and Data Augmentation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Used Keras image_dataset_from_directory to load images as datasets&lt;/li&gt;
&lt;li&gt;Added random flips and rotations for data augmentation&lt;/li&gt;
&lt;li&gt;Defined a pipeline that applies these data augmentation to each element of the training data set in parallel.&lt;/li&gt;
&lt;li&gt;I am also using &lt;code&gt;_prefetch_&lt;/code&gt; to avoid I/O blocking.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;fine-tuning-resnet50&#34;&gt;Fine-tuning ResNet50&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Used transfer learning with pretrained ResNet50 model&lt;/li&gt;
&lt;li&gt;Froze base layers, added GlobalAveragePooling and Dense layers&lt;/li&gt;
&lt;li&gt;Achieved &lt;strong&gt;98.46% test accuracy&lt;/strong&gt; after 5 epochs of fine-tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;img width=&#34;903&#34; alt=&#34;image&#34; src=&#34;https://github.com/gaurichaudhari9/image-classification-cnn/assets/25304556/368f5c47-9ddd-4c0b-bd51-79b38c3fcaa0&#34;&gt;
&lt;img width=&#34;1138&#34; alt=&#34;image&#34; src=&#34;https://github.com/gaurichaudhari9/image-classification-cnn/assets/25304556/6b3a9ecb-9306-4298-ae38-dcf4016a1ad3&#34;&gt;
&lt;h2 id=&#34;training-cnn-from-scratch&#34;&gt;Training CNN from Scratch&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Built a CNN model with convolutional, pooling, batch norm, dropout layers&lt;/li&gt;
&lt;li&gt;Trained for 10 epochs using Adam optimizer&lt;/li&gt;
&lt;li&gt;Achieved &lt;strong&gt;70%&lt;/strong&gt; test accuracy&lt;/li&gt;
&lt;li&gt;Training a model from scratch on a small dataset makes it difficult to learn robust features compared to fine-tuning a pretrained model. More data would be needed to improve accuracy.&lt;/li&gt;
&lt;/ul&gt;
&lt;img width=&#34;653&#34; alt=&#34;image&#34; src=&#34;https://github.com/gaurichaudhari9/image-classification-cnn/assets/25304556/026072da-1c90-40f0-92e4-0c5616043d96&#34;&gt;
&lt;img width=&#34;1121&#34; alt=&#34;image&#34; src=&#34;https://github.com/gaurichaudhari9/image-classification-cnn/assets/25304556/94613fbd-466d-4004-b224-01b63ea2df8a&#34;&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This project demonstrated classifying images using CNNs. Fine-tuning a pretrained CNN model like ResNet50 achieved high accuracy given the small dataset size. Training a CNN from scratch produced decent but lower accuracy. Overall, the models were able to effectively classify the apparel, chair, and footwear images.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Dataset collected using a phone camera. Location: Indiana University Bloomington Campus&lt;/li&gt;
&lt;li&gt;Models built with TensorFlow/Keras&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
